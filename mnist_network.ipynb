{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ecc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.radn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x)\n",
    "                        for x,y in zip(sizes[:-1],sizes[1:])]\n",
    "        \n",
    "\n",
    "\n",
    "# net = Network([2,3,2])\n",
    "# intialises a network with layers 2 3 2\n",
    "\n",
    "\n",
    "    def sigmoid(z):\n",
    "        return 1.0 / (1.0 / (1.0 + np.exp(-z)))\n",
    "\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w, in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot((w, a) + b))\n",
    "            return a\n",
    "        \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Train network using mini-batch SGD. Training data is a list of tuples (x,y) representing input and label.\n",
    "        If test_data is provided then network will be evaluated against the test data after each epoch and print progress.\n",
    "        Enables tracking but slows training down significantly.\n",
    "        \"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            \n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0,n,mini_batch_size)\n",
    "            ]\n",
    "\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1}/{2}\".format(j, self.evaluate(test_data), n_test))\n",
    "\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Update network weights and biases by applying gradient descent using backpropagation to a single minibatch.\n",
    "        The mini_batch is a list of tuples (x,y), and eta is the learning rate.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        for x,y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            nabla_b = [nb + dnd for nb, dnd in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w,nw in zip(self.weights, nabla_w)]\n",
    "        \n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                        for b,nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "        def backprop(self):\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
